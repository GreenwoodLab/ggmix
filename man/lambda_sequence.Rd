% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/functions.R
\name{lambda_sequence}
\alias{lambda_sequence}
\title{Calculate Sequence of Tuning Parameters}
\usage{
lambda_sequence(x, y, eigenvalues, weights = NULL, lambda_min_ratio,
  epsilon = 1e-14, tol.kkt = 1e-09, eta_init = 0.5, nlambda = 100,
  scale_x = F, center_y = F)
}
\arguments{
\item{x}{input matrix, of dimension n x p; where n is the number of
observations and p are the number of predictors.}

\item{y}{response variable. must be a quantitative variable}

\item{weights}{Separate penalty factors can be applied to each coefficient.
This is a number that multiplies lambda to allow differential shrinkage,
and can be used to apply adaptive LASSO. Can be 0 for some variables, which
implies no shrinkage, and that variable is always included in the model.
Default is 1 for all variables (and implicitly infinity for variables
listed in exclude). Note: the penalty factors are internally rescaled to
sum to nvars, and the lambda sequence will reflect this change.}

\item{lambda_min_ratio}{Smallest value for lambda, as a fraction of
lambda.max, the (data derived) entry value (i.e. the smallest value for
which all coefficients are zero). The default depends on the sample size
nobs relative to the number of variables nvars. If nobs > nvars, the
default is 0.0001, close to zero. If nobs < nvars, the default is 0.01. A
very small value of lambda.min.ratio will lead to a saturated fit in the
nobs < nvars case.}

\item{epsilon}{Convergence threshold for block relaxation of the entire
parameter vector \eqn{\Theta = ( \beta, \eta, \sigma^2 )}. The algorithm
converges when \deqn{crossprod(\Theta_{j+1} - \Theta_{j}) < \epsilon}.
Defaults value is 1E-7}

\item{eta_init}{initial value for the eta parameter. used in determining
lambda.max}

\item{nlambda}{the number of lambda values - default is 100.}

\item{scale_x}{should the columns of x be scaled - default is FALSE}

\item{center_y}{should y be mean centered - default is FALSE}

\item{lambda.factor}{The factor for getting the minimal lambda in lambda
sequence, where \code{min(lambda) = lambda.factor * max(lambda).
max(lambda)} is the smallest value of lambda for which all coefficients are
zero. The default depends on the relationship between \code{N} (the number
of rows in the matrix of predictors) and \code{p} (the number of
predictors). If \code{N > p}, the default is \code{1e-6}, close to zero. If
\code{N < p}, the default is \code{0.01}. A very small value of
lambda.factor will lead to a saturated fit.}
}
\value{
numeric vector of length \code{q}
}
\description{
Function to calculate the sequence of tuning parameters based on
  the design matrix \code{x} and the response variable {y}. This is used in
  the \code{\link{shim_once}} function to calculate the tuning parameters
  applied to the main effects
}
\details{
The maximum lambda is calculated using the following inequality:
  \deqn{(N*w_j)^-1 | \sum x_ij y_i | \le \lambda_max}

  The minimum lambda is given by lambda.factor*lambda_max. The sequence of
  nlambda values are decreasing from lambda_max to lambda_min on the log
  scale.

  The penalty factors are internally rescaled to sum to the number of
  predictor variables in glmnet. Therefore, to get the correct sequence of
  lambdas when there are weights, this function first rescales the weights
  and then calclated the sequence of lambdas.

  This formula is taken from section 2.5 of the \code{glmnet} paper in the
  Journal of Statistical Software (see references for details)
}
\examples{
# number of observations
n <- 100

# number of predictors
p <- 5

# environment variable
e <- sample(c(0,1), n, replace = T)

# main effects
x <- cbind(matrix(rnorm(n*p), ncol = p), e)

# need to label columns
dimnames(x)[[2]] <- c(paste0("x",1:p), "e")

# design matrix without intercept
X <- model.matrix(~(x1+x2+x3+x4+x5)*e-1, data = as.data.frame(x))

# response
Y <- X \%*\% rbinom(ncol(X), 1, 0.2) + 3*rnorm(n)

lambda_sequence(X,Y)
}
\references{
Friedman, J., Hastie, T. and Tibshirani, R. (2008)
  \emph{Regularization Paths for Generalized Linear Models via Coordinate
  Descent}, \url{http://www.stanford.edu/~hastie/Papers/glmnet.pdf}
  \emph{Journal of Statistical Software, Vol. 33(1), 1-22 Feb 2010}
  \url{http://www.jstatsoft.org/v33/i01/}

  Yang, Y., & Zou, H. (2015). A fast unified algorithm for solving
  group-lasso penalize learning problems. \emph{Statistics and Computing},
  25(6), 1129-1141.
  \url{http://www.math.mcgill.ca/yyang/resources/papers/gglasso.pdf}
}
\author{
Sahir Bhatnagar

Maintainer: Sahir Bhatnagar \email{sahir.bhatnagar@mail.mcgill.ca}
}
